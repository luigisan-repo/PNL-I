{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de lenguaje natural\n",
    "\n",
    "### Alumno\n",
    "- Crear sus propios vectores con Gensim basado en lo visto en clase con otro dataset.\n",
    "- Probar términos de interés y explicar similitudes en el espacio de embeddings (sacar conclusiones entre palabras similitudes y diferencias).\n",
    "- Utilizar modelos \n",
    "    - Skip-Gram - Entrenamiento\n",
    "    - CBOW - Entrenamiento\n",
    "- Graficarlos.\n",
    "- Obtener conclusiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### contenido\n",
    "\n",
    "El presente notebook se centra en la práctica de embeddings de palabras utilizando la librería Gensim y el algoritmo Word2Vec sobre un corpus literario de ciencia ficción la Trilogía de la Fundación de Isaac Asimov.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Windows-11-10.0.26100-SP0\n",
      "Python 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n",
      "Bits 64\n",
      "NumPy 1.26.4\n",
      "SciPy 1.13.1\n",
      "gemsim 4.3.3\n",
      "tensorflow 2.18.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nRecomendado para plataforma Windows-11-10.0.26100-SP0\\nPython 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\\nBits 64\\nNumPy 1.26.4\\nSciPy 1.13.1\\ngemsim 4.3.3\\ntensorflow 2.19.0\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform; print(platform.platform())\n",
    "import sys; print(\"Python\", sys.version)\n",
    "import struct; print(\"Bits\", 8 * struct.calcsize(\"P\"))\n",
    "import numpy; print(\"NumPy\", numpy.__version__)\n",
    "import scipy; print(\"SciPy\", scipy.__version__)\n",
    "import gensim; print(\"gemsim\", gensim.__version__)\n",
    "import tensorflow ; print (\"tensorflow\", tensorflow.__version__)\n",
    "\n",
    "\"\"\" \n",
    "Recomendado para plataforma Windows-11-10.0.26100-SP0\n",
    "Python 3.12.10 (tags/v3.12.10:0cc8128, Apr  8 2025, 12:21:36) [MSC v.1943 64 bit (AMD64)]\n",
    "Bits 64\n",
    "NumPy 1.26.4\n",
    "SciPy 1.13.1\n",
    "gemsim 4.3.3\n",
    "tensorflow 2.19.0\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZd5yLnnHOK0"
   },
   "source": [
    "<div style=\"text-align: right;\">\n",
    "\t<h1 style=\"display: inline-block;margin: 0;padding: 8px 16px;color: white;background: linear-gradient(to right,rgb(17, 75, 141), #4CAF50);border-radius: 12px;font-size: 1.8rem;box-shadow: 0 4px 10px rgba(0, 0, 0, 0.3);\">nota</h1>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "## Custom embedddings con Gensim\n",
    "\n",
    "## Conceptos Introductorios en PLN: Embeddings, Word2Vec y Gensim\n",
    "\n",
    "Para que las computadoras puedan procesar y \"entender\" el lenguaje humano, las palabras deben transformarse en un formato numérico. Aquí es donde entran en juego los **embeddings**.\n",
    "\n",
    "### 1. Embeddings de Palabras (Introducción General)\n",
    "\n",
    "- **¿Qué son?** Los embeddings de palabras son **representaciones numéricas densas** de palabras, donde cada palabra se convierte en un **vector** (una lista de números decimales, como `[0.123, -0.456, 0.789, ...]`). Estos vectores suelen tener una dimensionalidad mucho menor (ej., 50, 100, 300) que el vocabulario completo, lo que los hace eficientes.\n",
    "    \n",
    "- **La Idea Clave:** La magia de los embeddings radica en que **palabras con significados similares o que aparecen en contextos parecidos se ubican cerca unas de otras** en este espacio vectorial multidimensional. La \"distancia\" (ej., similitud coseno) y la \"dirección\" entre estos vectores capturan relaciones semánticas (ej., \"rey\" está cerca de \"reina\") y sintácticas.\n",
    "    \n",
    "- **¿Para qué se utilizan?** Permiten que los algoritmos de Machine Learning (que solo entienden números) trabajen con texto de una manera más rica y significativa que los métodos tradicionales como la \"bolsa de palabras\". Son fundamentales para tareas modernas de PLN como análisis de sentimiento, traducción, clasificación de texto, y más.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### 2. Word2Vec\n",
    "\n",
    "- **¿Qué es?** **Word2Vec** es un **algoritmo popular** (y una familia de modelos) desarrollado por Google que se utiliza para **aprender embeddings de palabras**. No es un embedding en sí mismo, sino el método para crearlos. Se entrena en grandes colecciones de texto (corpus) y aprende a asociar palabras con sus contextos.\n",
    "    \n",
    "- **¿Cómo funciona (idea simplificada)?** Word2Vec se basa en la \"hipótesis distribucional\": las palabras que aparecen en contextos similares tienden a tener significados similares. Internamente, utiliza redes neuronales poco profundas para dos tareas principales:\n",
    "    \n",
    "    - **Skip-gram:** Dado una palabra, predice las palabras de su contexto (las palabras que la rodean).\n",
    "        \n",
    "    - **CBOW (Continuous Bag-of-Words):** Dado el contexto de una palabra (las palabras que la rodean), predice la palabra central.\n",
    "        \n",
    "- **Resultado:** Después de entrenar Word2Vec en un corpus, el resultado final es un \"modelo\" que contiene los embeddings (vectores) para cada palabra en el vocabulario del corpus.\n",
    "    \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Gensim\n",
    "\n",
    "- **¿Qué es?** **Gensim** es una **librería de Python** especializada en el modelado de temas (Topic Modeling), la similitud de documentos y, muy importante para nuestro contexto, la **creación y manipulación de embeddings de palabras** (como Word2Vec).\n",
    "    \n",
    "- **Características Principales:**\n",
    "    \n",
    "    - **Eficiencia:** Está diseñada para manejar grandes volúmenes de texto de manera eficiente, lo cual es crucial en PLN.\n",
    "        \n",
    "    - **Implementación de Algoritmos:** Proporciona implementaciones de algoritmos clave como LDA, LSI, y por supuesto, Word2Vec y FastText.\n",
    "        \n",
    "    - **Simplicidad:** Ofrece una interfaz relativamente sencilla para trabajar con estos complejos modelos.\n",
    "        \n",
    "- **¿Para qué se utiliza?**\n",
    "    \n",
    "    - Entrenar tus propios modelos Word2Vec, FastText o Doc2Vec a partir de tu propio corpus de texto.\n",
    "        \n",
    "    - Cargar y utilizar modelos de embeddings pre-entrenados (como los de Google News que contienen embeddings de Word2Vec).\n",
    "        \n",
    "    - Realizar tareas de modelado de temas como Latent Dirichlet Allocation (LDA).\n",
    "        \n",
    "    - Calcular la similitud entre palabras o documentos utilizando los embeddings aprendidos.\n",
    "        \n",
    "\n",
    "---\n",
    "\n",
    "### 4. Embeddings con Gensim\n",
    "\n",
    "- **¿Qué significa?** \"Embeddings con Gensim\" se refiere al **proceso de generar y trabajar con embeddings de palabras o documentos utilizando la librería Gensim**. Gensim facilita todo el ciclo de vida de los embeddings:\n",
    "    \n",
    "    1. **Preparación del Corpus:** Gensim ayuda a preprocesar tu texto (tokenización, construcción de vocabulario, etc.) en un formato que sus modelos puedan entender.\n",
    "        \n",
    "    2. **Entrenamiento del Modelo:** Utilizas clases como `gensim.models.Word2Vec` o `gensim.models.FastText` para entrenar un modelo de embeddings en tu corpus. Durante este entrenamiento, el algoritmo aprende los vectores de las palabras.\n",
    "        \n",
    "    3. **Acceso a los Embeddings:** Una vez entrenado, el objeto modelo de Gensim te permite acceder a los vectores (embeddings) de las palabras (`model.wv['palabra_ejemplo']`).\n",
    "        \n",
    "    4. **Operaciones con Embeddings:** Gensim te proporciona métodos convenientes para realizar operaciones comunes con los embeddings, como encontrar las palabras más similares (`model.wv.most_similar('palabra')`), realizar analogías, etc.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "lFToQs5FK5uZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# PNL\n",
    "import multiprocessing\n",
    "# que es Word2Vec\n",
    "from gensim.models import Word2Vec\n",
    "#Complemento \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g07zJxG7H9vG"
   },
   "source": [
    "# Custom embedddings con Gensim -> Word2Vec\n",
    "\n",
    "## Carga del DataSet \n",
    "\n",
    "Dentro de la carpeta  `dataset_novelas-txt` tenemos nuestro corpus en idioma español y formato .txt .\n",
    "\n",
    "Para nuestro ejercicio escogemos uno de los archivos .txt como dataset. Los archivos están estructurados con un párrafo por línea y sin líneas vacías. \n",
    "\n",
    "En código a continuación  utilizamos `sep='/n'` para dividir el texto por saltos de línea permitiendo tratar cada párrafo como un documento independiente, de esta forma tendremos suficiente contexto semántico y el modelo Word2Vec puede aprender las relaciones entre palabras y generar word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "mysGrIw9ljC2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SAN\\AppData\\Local\\Temp\\ipykernel_24628\\2829754718.py:3: ParserWarning:\n",
      "\n",
      "Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# cargamos el Dataset \n",
    "os.listdir(\"./dataset_novelas-txt/\")\n",
    "df = pd.read_csv('./dataset_novelas-txt/Trilogía-Fundación-_Ed.-ilustrada_-Isaac-Asimov.txt', sep='/n', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA del dataset \n",
    "Realizamos un pequeño análisis exploratorio para conocer parte del contenido del dataset (archivo .txt) donde vemos :\n",
    "- Esta conformado por un dataset de texto sin etiquetas tomamos en cuenta que el Word2Vec no necesita etiquetas, para un aprendizaje no supervisado\n",
    "- Todo el corpus esta dividido en 8455 documentos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "LEpKubK9XzXN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*. Se encuentra conformado por 8455 documentos\n"
     ]
    }
   ],
   "source": [
    "# Cantidad de documentos \n",
    "print(f\"*. Se encuentra conformado por {df.shape[0]} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "ticoqYD1Z3I7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*. los primero 5 documentos son : \n",
      "                                                    0\n",
      "0  Isaac Asimov empezó a escribir la saga de la F...\n",
      "1  ¿Por qué surgen y caen los imperios? La Trilog...\n",
      "2  Esta edición única de la Trilogía Fundación, i...\n",
      "3                                       Isaac Asimov\n",
      "4                                 Trilogía Fundación\n"
     ]
    }
   ],
   "source": [
    "# mostramos los primeros 5 documentos \n",
    "print (f\"*. los primero 5 documentos son : \\n {df.head()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del dataset:\n",
      "- Forma: (8455, 1)\n",
      "- Columnas: [0]\n",
      "- Tipo de datos: 0    object\n",
      "dtype: object\n",
      "- Longitud promedio por documento: 157.1 caracteres\n",
      "- Documento más corto: 1 caracteres\n",
      "- Documento más largo: 1255 caracteres\n",
      "\n",
      "Tipos de contenido encontrados:\n",
      "Documentos cortos (probablemente metadatos):\n",
      "3                                      Isaac Asimov\n",
      "4                                Trilogía Fundación\n",
      "5                                 Edición Ilustrada\n",
      "6                                         ePub r1.0\n",
      "7                                Watcher 01-10-2022\n",
      "8           Título original: The Foundation Trilogy\n",
      "9                                Isaac Asimov, 1963\n",
      "10    Traducción: Manuel de los Reyes García Campos\n",
      "12                            Colección NOVA nº 337\n",
      "13                          Editor digital: Watcher\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# estructura del  dataset:\n",
    "print(\"Estructura del dataset:\")\n",
    "print(f\"- Forma: {df.shape}\")\n",
    "print(f\"- Columnas: {df.columns.tolist()}\")\n",
    "print(f\"- Tipo de datos: {df.dtypes}\")\n",
    "\n",
    "# Análisis de longitud de documentos\n",
    "df['longitud'] = df[0].str.len()\n",
    "print(f\"- Longitud promedio por documento: {df['longitud'].mean():.1f} caracteres\")\n",
    "print(f\"- Documento más corto: {df['longitud'].min()} caracteres\")\n",
    "print(f\"- Documento más largo: {df['longitud'].max()} caracteres\")\n",
    "\n",
    "# Ejemplos de diferentes tipos de contenido\n",
    "print(\"\\nTipos de contenido encontrados:\")\n",
    "print(\"Documentos cortos (probablemente metadatos):\")\n",
    "print(df[df['longitud'] < 50][0].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85     1\n",
      "111    2\n",
      "150    3\n",
      "218    4\n",
      "277    5\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# muestra 5 documentos que contengan menos de 1 carácter \n",
    "print(df[df['longitud'] < 2][0].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documentos que se encuentran entre las posiciones 200 al 205: \n",
      "                                                      0  longitud\n",
      "149  En todo este tiempo, desde el momento del dese...       105\n",
      "150                                                  3         1\n",
      "151  TRANTOR: […] Fue a comienzos del decimotercer ...       439\n"
     ]
    }
   ],
   "source": [
    "# consulta de documentos por posición \n",
    "print(f\"Documentos que se encuentran entre las posiciones 200 al 205: \\n {df[149:152]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ab94qaFlrA1G"
   },
   "source": [
    "## Preprocesamiento\n",
    "\n",
    "Realizamos limpieza de los datos donde utilizamos el `text_to_word_sequence` para convertir la cadena de texto en una lista de palabras en minúscula y sin puntuación ejemplo \n",
    "-  \"Fundación\" y \"fundación\" ------>  \"fundacion\"\n",
    "- \"exclamó\" -----> exclamo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "rIsmMWmjrDHd"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "sentence_tokens = []\n",
    "# Recorrer todas las filas y transformar las oraciones\n",
    "# en una secuencia de palabras (esto podría realizarse con NLTK o spaCy también)\n",
    "for _, row in df[:None].iterrows():\n",
    "    sentence_tokens.append(text_to_word_sequence(row[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "CHepi_DGrbhq"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['isaac',\n",
       "  'asimov',\n",
       "  'empezó',\n",
       "  'a',\n",
       "  'escribir',\n",
       "  'la',\n",
       "  'saga',\n",
       "  'de',\n",
       "  'la',\n",
       "  'fundación',\n",
       "  'a',\n",
       "  'los',\n",
       "  'veintiún',\n",
       "  'años',\n",
       "  'sin',\n",
       "  'saber',\n",
       "  'que',\n",
       "  'un',\n",
       "  'día',\n",
       "  'esta',\n",
       "  'obra',\n",
       "  'se',\n",
       "  'convertiría',\n",
       "  'en',\n",
       "  'la',\n",
       "  'piedra',\n",
       "  'angular',\n",
       "  'de',\n",
       "  'la',\n",
       "  'ciencia',\n",
       "  'ficción',\n",
       "  'del',\n",
       "  'siglo',\n",
       "  'xx',\n",
       "  'ni',\n",
       "  'que',\n",
       "  'con',\n",
       "  'ella',\n",
       "  'cautivaría',\n",
       "  'a',\n",
       "  'lectores',\n",
       "  'de',\n",
       "  'todas',\n",
       "  'las',\n",
       "  'edades',\n",
       "  'durante',\n",
       "  'más',\n",
       "  'de',\n",
       "  'siete',\n",
       "  'décadas']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Dame todos los elementos de la lista sentence_tokens desde el principio (índice 0) hasta el índice 1, sin incluir el elemento en el índice 1.\"\n",
    "sentence_tokens[:1]\n",
    "#sentence_tokens[2:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El número total de palabras en 'sentence_tokens' es: 221985 (tomando en cuenta la duplicidad)\n"
     ]
    }
   ],
   "source": [
    "total_palabras = 0\n",
    "for sentence in sentence_tokens:\n",
    "    total_palabras += len(sentence)\n",
    "\n",
    "print(f\"El número total de palabras en 'sentence_tokens' es: {total_palabras} (tomando en cuenta la duplicidad)\")\n",
    "\n",
    "## Otra forma para contar la lista de palabras sum()\n",
    "#total_palabras_conciso = sum(len(sentence) for sentence in sentence_tokens)\n",
    "#print(f\"El número total de palabras  es: {total_palabras_conciso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BaXV6nlHr5Aa"
   },
   "source": [
    "## Creación del modelo Word2Vec\n",
    "\n",
    "\n",
    "En el siguiente bloque de código definimos un callback personalizado para los modelos de embeddings de Gensim (Word2Vec). Su propósito principal es monitorear y reportar la pérdida (loss) de entrenamiento en cada época durante el proceso de entrenamiento del modelo, ya que Gensim no lo hace por defecto de una manera fácilmente visible.\n",
    "\n",
    "\n",
    "La pérdida (loss) es un valor numérico que cuantifica el error o la inexactitud de las predicciones de un modelo. Cuanto mayor sea su valor, peores serán las predicciones del modelo; inversamente, cuanto menor sea la pérdida, mejor será la predicción.\n",
    "\n",
    "\n",
    "**Cómo funciona según la arquitectura**\n",
    "\n",
    "|Arquitectura|Tarea de Predicción|Función de Pérdida Evalúa|\n",
    "|---|---|---|\n",
    "|**Skip-gram**|Predice palabras de contexto → objetivo|Efectividad al predecir vecinos de una palabra central|\n",
    "|**CBOW**|Predice palabra central ← contexto|Efectividad al predecir palabra central desde el contexto|\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "OSb0v7h8r7hK"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Durante el entrenamiento gensim por defecto no informa el \"loss\" en cada época\n",
    "# Sobrecargamos el callback para poder tener esta información\n",
    "class callback(CallbackAny2Vec):\n",
    "    \"\"\"\n",
    "    Callback to print loss after each epoch\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name=\"\"):\n",
    "        self.epoch = 0\n",
    "        self.model_name = model_name  # Para identificar qué modelo está entrenando\n",
    "        self.loss_previous_step = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "         #  Este método de Gensim te devuelve la pérdida acumulada desde la última vez que el estado de entrenamiento del modelo fue \"reiniciado\"\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print(f'{self.model_name} - Loss after epoch {self.epoch}: {loss}')\n",
    "        else:\n",
    "            print(f'{self.model_name} - Loss after epoch {self.epoch}: {loss - self.loss_previous_step}')\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddT9NVuNlCAe"
   },
   "source": [
    "### Entrenar embeddings\n",
    "\n",
    "Utilizaremos Word2Vec, un algoritmo para aprender embeddings de palabras (vectores numéricos densos), a partir de nuestro corpus de texto. Este proceso generará representaciones vectoriales para cada palabra. \n",
    "\n",
    "\n",
    "<b id=\"Word2Vec\">Word2Vec tiene dos arquitecturas</b> principales para aprender las representaciones vectoriales de las palabras en este ejercicio utilizaremos:\n",
    "\n",
    "1. **CBOW (Continuous Bag-of-Words):**\n",
    "    \n",
    "    - **Funcionamiento:** Toma las palabras que rodean a una palabra central (el **contexto**) como entrada y predice la **palabra central**.\n",
    "        \n",
    "    - **Ventajas:** Es más rápido de entrenar y funciona muy bien con palabras frecuentes.\n",
    "        \n",
    "    - **Parámetro en Gensim:** `sg = 0` (el valor por defecto).\n",
    "        \n",
    "2. **Skip-gram:**\n",
    "    \n",
    "    - **Funcionamiento:** Toma una **palabra central** como entrada y predice las **palabras de contexto** que la rodean. Es el opuesto de CBOW.\n",
    "        \n",
    "    - **Ventajas:** Es mejor para capturar relaciones semánticas complejas y funciona bien con palabras raras o que aparecen con poca frecuencia.\n",
    "        \n",
    "    - **Parámetro en Gensim:** `sg = 1`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENTRENANDO MODELO SKIP-GRAM (sg=1)\n",
      "Skip-gram - Cantidad de documentos en el corpus: 8455\n",
      "Skip-gram - Cantidad de palabras en vocabulario: 4644\n",
      "SKIP-GRAM - Loss after epoch 0: 1782827.0\n",
      "SKIP-GRAM - Loss after epoch 1: 1167566.5\n",
      "SKIP-GRAM - Loss after epoch 2: 1110558.0\n",
      "SKIP-GRAM - Loss after epoch 3: 1046721.0\n",
      "SKIP-GRAM - Loss after epoch 4: 1027282.5\n",
      "SKIP-GRAM - Loss after epoch 5: 1017644.5\n",
      "SKIP-GRAM - Loss after epoch 6: 1002948.0\n",
      "SKIP-GRAM - Loss after epoch 7: 967019.5\n",
      "SKIP-GRAM - Loss after epoch 8: 949066.0\n",
      "SKIP-GRAM - Loss after epoch 9: 940482.0\n",
      "SKIP-GRAM - Loss after epoch 10: 931070.0\n",
      "SKIP-GRAM - Loss after epoch 11: 922380.0\n",
      "SKIP-GRAM - Loss after epoch 12: 912506.0\n",
      "SKIP-GRAM - Loss after epoch 13: 906706.0\n",
      "SKIP-GRAM - Loss after epoch 14: 901101.0\n",
      "SKIP-GRAM - Loss after epoch 15: 895594.0\n",
      "SKIP-GRAM - Loss after epoch 16: 872002.0\n",
      "SKIP-GRAM - Loss after epoch 17: 860788.0\n",
      "SKIP-GRAM - Loss after epoch 18: 855154.0\n",
      "SKIP-GRAM - Loss after epoch 19: 855004.0\n",
      "ENTRENAMIENTO COMPLETADO\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# modelo Word2Vec\n",
    "print(\"=\"*60)\n",
    "print(\"ENTRENANDO MODELO SKIP-GRAM (sg=1)\")\n",
    "\n",
    "\n",
    "# Modelo 1: Skip-gram (sg=1)\n",
    "w2v_skipgram = Word2Vec(min_count=5,        #  Ignora palabras que aparecen menos de 5 veces (reduce ruido)\n",
    "                        window=2,           # Contexto de 2 palabras a cada lado (antes y después como contexto )\n",
    "                        vector_size=300,    # Cada palabra será un vector de 300 dimensiones\n",
    "                        negative=20,        # Técnica de optimización (muestreo negativo)\n",
    "                        workers=1,          # cuanto CPU utilizar \n",
    "                        sg=1)  # ARQUITECTURA/TIPO Skip-gram\n",
    "\n",
    "# pasamos el vocabulario al modelo\n",
    "w2v_skipgram.build_vocab(sentence_tokens)\n",
    "\n",
    "print(f\"Skip-gram - Cantidad de documentos en el corpus: {w2v_skipgram.corpus_count}\")\n",
    "print(f\"Skip-gram - Cantidad de palabras en vocabulario: {len(w2v_skipgram.wv.index_to_key)}\")\n",
    "\n",
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_skipgram.train(sentence_tokens,\n",
    "                   total_examples=w2v_skipgram.corpus_count,\n",
    "                   epochs=20,\n",
    "                   compute_loss=True,\n",
    "                   callbacks=[callback(\"SKIP-GRAM\")])\n",
    "\n",
    "print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENTRENANDO MODELO CBOW (sg=0)\n",
      "CBOW - DCantidad de documentos en el corpus: 8455\n",
      "CBOW - Cantidad de palabras en vocabulario: 4644\n",
      "CBOW - Loss after epoch 0: 766093.75\n",
      "CBOW - Loss after epoch 1: 543693.5\n",
      "CBOW - Loss after epoch 2: 494966.375\n",
      "CBOW - Loss after epoch 3: 449346.875\n",
      "CBOW - Loss after epoch 4: 397442.75\n",
      "CBOW - Loss after epoch 5: 389348.0\n",
      "CBOW - Loss after epoch 6: 380808.5\n",
      "CBOW - Loss after epoch 7: 374795.5\n",
      "CBOW - Loss after epoch 8: 368295.25\n",
      "CBOW - Loss after epoch 9: 341653.5\n",
      "CBOW - Loss after epoch 10: 335403.5\n",
      "CBOW - Loss after epoch 11: 330135.5\n",
      "CBOW - Loss after epoch 12: 326310.0\n",
      "CBOW - Loss after epoch 13: 322524.5\n",
      "CBOW - Loss after epoch 14: 319547.5\n",
      "CBOW - Loss after epoch 15: 317019.5\n",
      "CBOW - Loss after epoch 16: 314050.5\n",
      "CBOW - Loss after epoch 17: 312077.5\n",
      "CBOW - Loss after epoch 18: 310907.0\n",
      "CBOW - Loss after epoch 19: 308851.5\n",
      "ENTRENAMIENTO COMPLETADO\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ENTRENANDO MODELO CBOW (sg=0)\")\n",
    "\n",
    "# Modelo 2: CBOW (sg=0)\n",
    "w2v_cbow = Word2Vec(min_count=5,    \n",
    "                    window=2,       \n",
    "                    vector_size=300,       \n",
    "                    negative=20,    \n",
    "                    workers=1,   \n",
    "                    sg=0)  # CBOW\n",
    "\n",
    "w2v_cbow.build_vocab(sentence_tokens)\n",
    "\n",
    "print(f\"CBOW - DCantidad de documentos en el corpus: {w2v_cbow.corpus_count}\")\n",
    "print(f\"CBOW - Cantidad de palabras en vocabulario: {len(w2v_cbow.wv.index_to_key)}\")\n",
    "# Entrenamos el modelo generador de vectores\n",
    "# Utilizamos nuestro callback\n",
    "w2v_cbow.train(sentence_tokens,\n",
    "               total_examples=w2v_cbow.corpus_count,\n",
    "               epochs=20,\n",
    "               compute_loss=True,\n",
    "               callbacks=[callback(\"CBOW\")])\n",
    "\n",
    "print(\"ENTRENAMIENTO COMPLETADO\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploración de similitudes \n",
    "\n",
    "\n",
    "- Positive: Encuentra palabras  que aparecen en contexto similares a \"imperio\"\n",
    "- Negative: Encuentra palabras  que aparecen en contexto diferentes a \"imperio\"\n",
    "\n",
    "Después del entrenamiento, cada palabra tiene un vector de 300 números que captura su \"significado\" basado en sus contextos. Palabras con significados similares tendrán vectores similares(cercano) en el espacio multidimensional.\n",
    "\n",
    "Word2Vec aprendió que \"imperio\", \"espíritu\", \"galáctico\" aparecen en contextos similares de acuerdo al dataset (Fundación), por lo que sus vectores estarán cerca en el espacio vectorial en Word2Vec\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulta de similitudes positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "6cHN9xGLuPEm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('espíritu', 0.6739826202392578),\n",
       " ('galáctico', 0.5707151889801025),\n",
       " ('universal', 0.5567495822906494),\n",
       " ('fundacionista', 0.5410391688346863),\n",
       " ('lugar', 0.5362286567687988)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelo SKIP-GRAM -> Similitudes positivas, encontramos 5 palabras similares \n",
    "w2v_skipgram.wv.most_similar(positive=[\"imperio\"], topn=5)\n",
    "# Mientras el resultado este mas cercano a 1 será mas fuerte la relación del valor de Similitud coseno\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('espíritu', 0.8241751790046692),\n",
       " ('fundacionista', 0.6728864908218384),\n",
       " ('reino', 0.6250720024108887),\n",
       " ('universo', 0.6210659146308899),\n",
       " ('modelo', 0.6205688714981079)]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelo CBOW -> Similitudes positivas, encontramos 5 palabras similares \n",
    "w2v_cbow.wv.most_similar(positive=[\"imperio\"], topn=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, ambos modelos **Skip-gram** y **CBOW** arrojan resultados con algunas palabras distintas o en distinto orden (\"espíritu\" , \"fundacionista\"). Ambos modelos aprenden de manera diferente, por lo que generan vectores distintos para las mismas palabras\n",
    "\n",
    "\n",
    "**Proceso de Aprendizaje**\n",
    "Skip-gram (sg=1):\n",
    "- Entrada: Una palabra central (\"imperio\")\n",
    "- Salida: Predice las palabras del contexto circundante\n",
    "- Aprende: \"Si veo 'imperio', ¿qué palabras espero encontrar alrededor?\"\n",
    "- Sensibilidad al contexto: Más sensible a palabras frecuentes en contextos específicos\n",
    "- Skip-gram: Mejor para palabras específicas/raras\n",
    "\n",
    "CBOW (sg=0):\n",
    "- Entrada: Palabras del contexto circundante\n",
    "- Salida: Predice la palabra central\n",
    "- Aprende: \"Si veo estas palabras alrededor, ¿cuál debería ser la palabra central?\"\n",
    "- Sensibilidad al contexto: Mejor para capturar el significado promedio del contexto\n",
    "- CBOW: Mejor para conceptos generales/frecuentes\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW similitud -> positive:\n",
      "psicohistoria: [('psicología', 0.8443653583526611), ('historia', 0.7844677567481995), ('convención', 0.7822109460830688), ('ciencia', 0.7587755918502808), ('predecir', 0.7432090640068054)]\n",
      "fundacionista: [('soldado', 0.8334208726882935), ('mero', 0.82328200340271), ('modelo', 0.8183872699737549), ('plano', 0.8164640069007874), ('título', 0.8098716735839844)]\n",
      "espíritu: [('modelo', 0.833525538444519), ('imperio', 0.8241754174232483), ('emperador', 0.8093100786209106), ('acorazado', 0.8026908040046692), ('fundacionista', 0.8019493818283081)]\n",
      "planeta: [('mundo', 0.7795717716217041), ('reino', 0.7774387001991272), ('sector', 0.7649549245834351), ('sistema', 0.7637091875076294), ('centro', 0.7297593951225281)]\n",
      "\n",
      "Skip-gram similitud -> positive:\n",
      "psicohistoria: [('predecir', 0.7623522877693176), ('validez', 0.7321229577064514), ('psicología', 0.7251515984535217), ('convención', 0.7032368183135986), ('leyes', 0.7030066251754761)]\n",
      "fundacionista: [('converso', 0.7961498498916626), ('espía', 0.7937206625938416), ('traidor', 0.793029248714447), ('burdo', 0.7916452884674072), ('súbdito', 0.7896440625190735)]\n",
      "espíritu: [('beneplácito', 0.7469762563705444), ('modelo', 0.7278505563735962), ('glorioso', 0.7212045788764954), ('antiguo', 0.7103278636932373), ('galáctico', 0.7041487693786621)]\n",
      "planeta: [('reino', 0.6765718460083008), ('esplendor', 0.6490373611450195), ('aislamiento', 0.64844810962677), ('sector', 0.6456514000892639), ('territorio', 0.6430009007453918)]\n"
     ]
    }
   ],
   "source": [
    "# similitud positiva, prueba con multiples palabras\n",
    "palabras_test = ['psicohistoria', 'fundacionista', 'espíritu', 'planeta']\n",
    "\n",
    "print(\"CBOW similitud -> positive:\")\n",
    "for palabra in palabras_test:\n",
    "    try:\n",
    "        # por defecto realizamos una similitud positiva \n",
    "        # w2v_cbow.wv.most_similar(positive=[palabra]\n",
    "        print(f\"{palabra}: {w2v_cbow.wv.most_similar(palabra, topn=5)}\")\n",
    "    except:\n",
    "        print(f\"{palabra}: no encontrada\")\n",
    "\n",
    "print(\"\\nSkip-gram similitud -> positive:\")\n",
    "for palabra in palabras_test:\n",
    "    try:\n",
    "        print(f\"{palabra}: {w2v_skipgram.wv.most_similar(palabra, topn=5)}\")\n",
    "    except:\n",
    "        print(f\"{palabra}: no encontrada\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consulta de similitudes negative\n",
    "\n",
    "Como lo comentamos anteriormente palabras que tiene una distancia vectorial mas alejada de la palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mientras', 0.02322714403271675),\n",
       " ('pausa', -0.02752688154578209),\n",
       " ('tenía', -0.0357431136071682),\n",
       " ('ojos', -0.04321027174592018),\n",
       " ('homir', -0.04343747720122337)]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Palabras que MENOS se relacionan con ....\n",
    "# Similitudes con relación negativas, palabras que conceptualmente opuestas o no relacionadas\n",
    "w2v_skipgram.wv.most_similar(negative=[\"imperio\"], topn=5)\n",
    "# valores de vectores mas alijados a la palabra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('—dijo', 0.2635144591331482),\n",
       " ('—replicó', 0.2433919608592987),\n",
       " ('—preguntó', 0.23636683821678162),\n",
       " ('—repuso', 0.22727835178375244),\n",
       " ('homir', 0.20920304954051971)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Palabras que MENOS se relacionan con ....\n",
    "# Similitudes con relación negativas, palabras que conceptualmente opuestas o no relacionadas\n",
    "w2v_cbow.wv.most_similar(negative=[\"imperio\"], topn=5)\n",
    "# valores de vectores mas alijados a la palabra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manejo de los vectores de las palabras.\n",
    "\n",
    "**Obtener los vectores de una palabra**: recordemos que anteriormente se configuro `vector_size=300` en lso 2 modelos por esa razón estamos viendo el vector completo de la palabra \"gobernante\" con  300 dimensiones. <a href=\"#Word2Vec\">Word2Vec arquitectura CBOW y Skip-gram</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estructura del vocabulario palabras x dimensiones: (4644, 300)\n",
      "Cantidad de vectores que tiene 'gobernante': 300\n"
     ]
    }
   ],
   "source": [
    "#Visualización de vectores en el modelo SKIP-GRAM\n",
    "#get_vector = w2v_skipgram.wv.get_vector(\"gobernante\")\n",
    "\n",
    "#w2v_cbow\n",
    "# Confirma el tamaño:\n",
    "print(\"Estructura del vocabulario palabras x dimensiones:\", w2v_skipgram.wv.vectors.shape)\n",
    "# Resultado: (4644, 300)\n",
    "#            ↑     ↑\n",
    "#       palabras  dimensiones\n",
    "\n",
    "print(\"Cantidad de vectores que tiene 'gobernante':\", len(w2v_skipgram.wv['gobernante']))\n",
    "# Resultado: 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La palabra 'gobernante' es la palabra #3064 de 4644, con 300 dimensiones\n"
     ]
    }
   ],
   "source": [
    "posicion = w2v_skipgram.wv.key_to_index[palabra]\n",
    "dimensiones = len(w2v_skipgram.wv[palabra])\n",
    "total_palabras = len(w2v_skipgram.wv.index_to_key)\n",
    "    \n",
    "print(f\"La palabra '{palabra}' es la palabra #{posicion + 1} de {total_palabras}, con {dimensiones} dimensiones\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra imperio  posición 56 dimenciones 300 Vector :\n",
      "[ 0.19409874  0.01573598  0.08015897 -0.03631405  0.15944874 -0.5250009\n",
      " -0.3347543   0.4282128  -0.09902585 -0.14503525 -0.06346194  0.02194023\n",
      " -0.25882226  0.28304267  0.18101196 -0.32378036 -0.22360295  0.31578523\n",
      " -0.18539077 -0.09086666  0.18001708  0.17867883 -0.32029477  0.20244502\n",
      " -0.03397013  0.3181885   0.03282682  0.06901656  0.11301483  0.12488827\n",
      "  0.2008269   0.08412392  0.5394682  -0.06675683  0.09548599  0.26610464\n",
      "  0.65936494  0.21876372  0.41565597  0.12254391  0.3163139  -0.49817804\n",
      " -0.07341616  0.4246808   0.01768071 -0.5370595   0.33464912  0.42576408\n",
      "  0.26745176  0.5678598  -0.15728584  0.26447973 -0.03237135 -0.12749626\n",
      " -0.00546669  0.3469397   0.03653575  0.05815914  0.03152079 -0.1996716\n",
      " -0.4110965  -0.15564004  0.07368261  0.22838435 -0.06387999  0.24067177\n",
      "  0.11521165  0.5796124   0.30342594  0.28354093  0.18727341 -0.2172573\n",
      " -0.30145246 -0.18071704  0.18207006  0.45042813 -0.44592774 -0.06757113\n",
      "  0.20246197  0.01295562 -0.3069895  -0.26369777  0.10881988  0.82446694\n",
      " -0.23982847 -0.26008403  0.27683455  0.250823   -0.33621058  0.1264672\n",
      "  0.05110417  0.01254271 -0.14900976  0.21353345  0.24627914  0.28797087\n",
      " -0.20836376  0.1342879  -0.23784548  0.09245063 -0.37960365  0.09284307\n",
      "  0.0618673  -0.25078976 -0.10156195  0.20169258 -0.11153086 -0.1439305\n",
      "  0.04914282 -0.26271713 -0.07507057  0.42673975 -0.10217462 -0.10052982\n",
      "  0.38495365  0.5783742   0.45516804 -0.339533   -0.31640348 -0.14455095\n",
      " -0.5998334   0.10628773  0.9074048  -0.20544013 -0.18518509  0.36785513\n",
      " -0.39783657 -0.2717799  -0.13498184 -0.20711681 -0.28913563  0.25387812\n",
      "  0.03412054 -0.22598836 -0.25840226 -0.04907454 -0.00703291 -0.03163068\n",
      " -0.32582483 -0.02199753  0.08064339 -0.72634816  0.133568    0.0229672\n",
      " -0.02617668 -0.44003364 -0.11920165 -0.34353822 -0.06920582 -0.42109743\n",
      "  0.28127956 -0.23152824 -0.10389895 -0.607334   -0.27267662 -0.11400931\n",
      "  0.10351906  0.32264334  0.30614978  0.2120387  -0.18207002 -0.22013806\n",
      " -0.47403768  0.272194    0.14554998 -0.27199727 -0.15644188  0.1759576\n",
      " -0.08185375  0.17613654 -0.6535705  -0.00512167 -0.30286536  0.49865168\n",
      "  0.1847282  -0.02361932  0.1610222  -0.03811105  0.09129553  0.22015147\n",
      " -0.34040493 -0.20810097 -0.05558304  0.07317846  0.1544513  -0.5272255\n",
      " -0.08982128 -0.12559769 -0.153008    0.02815333 -0.06430351 -0.07185308\n",
      " -0.3864665  -0.18195525  0.17695212 -0.01719349  0.04008014  0.14835685\n",
      "  0.35799116 -0.04593265  0.00306528  0.24162666 -0.09311028 -0.45051938\n",
      "  0.42418543 -0.38172233  0.39889482  0.1044099  -0.3771645  -0.3856809\n",
      "  0.33818546 -0.10245015  0.22713701 -0.4914777   0.1503932   0.45632252\n",
      "  0.01501406 -0.50876796  0.02519549  0.32120487 -0.1925561  -0.06965626\n",
      "  0.01368478 -0.39941803 -0.20144041  0.03455992 -0.11564524 -0.42247397\n",
      " -0.19088104 -0.08369882  0.1670238  -0.11181252 -0.16944821 -0.16120216\n",
      " -0.02782959  0.2736672  -0.18527472 -0.2826182   0.17114016 -0.488002\n",
      " -0.02502082 -0.33153778 -0.18560681  0.16430126  0.10850159 -0.35051534\n",
      " -0.30462667 -0.31125894  0.14068653  0.43926746  0.18991043  0.00785426\n",
      "  0.49395233 -0.42320007 -0.6161793  -0.16038911  0.10048725  0.48538235\n",
      " -0.02763438  0.3361312  -0.2998925   0.10404369 -0.02001901 -0.7298534\n",
      "  0.0602844   0.67706746  0.24822696  0.60362    -0.09703796  0.6154681\n",
      " -0.5934177  -0.31912637  0.37544474  0.4940252  -0.3498528   0.38884538\n",
      "  0.37824413  0.2060161  -0.15306096 -0.6579258   0.53115845  0.805823\n",
      " -0.04427325 -0.34716865 -0.32753786 -0.37504518 -0.16683817  0.07977641\n",
      " -0.0351091   0.40609673  0.16475306 -0.05249925 -0.03937737 -0.21851608\n",
      "  0.28448588  0.6122766  -0.03571403  0.23319776  0.17252274  0.07391533]\n"
     ]
    }
   ],
   "source": [
    "# modelo SKIP-GRAM\n",
    "palabra ='imperio'\n",
    "\n",
    "# el método `get_vector` permite obtener los vectores:\n",
    "get_vector = w2v_skipgram.wv.get_vector(palabra)\n",
    "print (f'Palabra {palabra}  posición {w2v_skipgram.wv.key_to_index[palabra]} dimenciones {len(w2v_skipgram.wv[palabra])} Vector :')\n",
    "print(get_vector)\n",
    "# Obtener el vector completo mostrando  el vector real de 300 números de \"gobernante\"\n",
    "# Estos números no son interpretables individualmente\n",
    "# Su valor está en las relaciones con otros vectores\n",
    "\n",
    "# modelo CBOW\n",
    "#get_vector = w2v_cbow.wv.get_vector(\"gobernante\")\n",
    "#print(get_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud usando el vector get_vector de la palabra \"imperio\" \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('imperio', 1.0),\n",
       " ('espíritu', 0.6739826202392578),\n",
       " ('galáctico', 0.5707152485847473),\n",
       " ('universal', 0.5567495822906494),\n",
       " ('fundacionista', 0.5410391688346863),\n",
       " ('lugar', 0.5362287759780884),\n",
       " ('restos', 0.5326494574546814),\n",
       " ('modelo', 0.5325555801391602),\n",
       " ('nyak', 0.5317217111587524),\n",
       " ('interregno', 0.5316949486732483)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (f'Similitud usando el vector get_vector de la palabra \"{palabra}\" ')\n",
    "# = \"gobernante\"\n",
    "# el método `most_similar` también permite comparar a partir de vectores\n",
    "w2v_skipgram.wv.most_similar(get_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('imperio', 0.8050969243049622),\n",
       " ('fundacionista', 0.6731754541397095),\n",
       " ('sumamente', 0.5267733335494995),\n",
       " ('converso', 0.5156847238540649),\n",
       " ('espíritu', 0.5053003430366516),\n",
       " ('—ahora', 0.502896249294281),\n",
       " ('—¡qué', 0.499081552028656),\n",
       " ('peligroso', 0.4981633126735687),\n",
       " ('esto…', 0.49484819173812866),\n",
       " ('—…', 0.49283191561698914)]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aritmética Vectorial en Word2Vec\n",
    "resultado = w2v_skipgram.wv.get_vector(\"imperio\") - w2v_skipgram.wv.get_vector(\"galáctico\") + w2v_skipgram.wv.get_vector(\"fundacionista\")\n",
    "w2v_skipgram.wv.most_similar([resultado])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cuando realizamos operaciones **Aritmética Vectorial en Word2Vec** podemos alterar las relaciones entre las palabras. como podemos ver  en nuestro ejemplo  anterior podemos observar el nivel de relación perteneciente a imperio.\n",
    "\n",
    "- [('imperio', 1.0),\n",
    "- ('galáctico', 0.5707152485847473),\n",
    "-  ('fundacionista', 0.5410391688346863),\n",
    "\n",
    "al aplicar aritmética vectorial en Word2Vec, donde : \n",
    "- [('imperio', 0.8050969243049622),\n",
    "- ('fundacionista', 0.6731754541397095),\n",
    "- \"imperio\" - \"galáctico\" = alejamos \"imperio\" y cambiando su valor vectorial de \"galáctico\"\n",
    "- \"imperio\" + \"fundacionista\" = el nuevo vector de \"imperio\" lo acerca a \"fundacionista\" obteniendo un nuevo espacio semántico. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FUNCIÓN QUE COMPARACIÓN PARA LA PALABRA: 'robot' ===\n",
      " La palabra 'robot' no está en el vocabulario de uno o ambos modelos\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== FUNCIÓN QUE COMPARACIÓN PARA LA PALABRA: 'galaxia' ===\n",
      "\n",
      " SKIP-GRAM (sg=1):\n",
      "   1. periferia       (similitud: 0.5833)\n",
      "   2. gente           (similitud: 0.5492)\n",
      "   3. galaxia»        (similitud: 0.5451)\n",
      "   4. opuesto         (similitud: 0.5285)\n",
      "\n",
      " CBOW (sg=0):\n",
      "   1. periferia       (similitud: 0.7901)\n",
      "   2. humanidad       (similitud: 0.6605)\n",
      "   3. opuesto         (similitud: 0.6300)\n",
      "   4. enciclopedia    (similitud: 0.6277)\n",
      "\n",
      " PALABRAS EN COMÚN: 2/4\n",
      "   ['opuesto', 'periferia']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== FUNCIÓN QUE COMPARACIÓN PARA LA PALABRA: 'planeta' ===\n",
      "\n",
      " SKIP-GRAM (sg=1):\n",
      "   1. reino           (similitud: 0.6766)\n",
      "   2. esplendor       (similitud: 0.6490)\n",
      "   3. aislamiento     (similitud: 0.6484)\n",
      "   4. sector          (similitud: 0.6457)\n",
      "\n",
      " CBOW (sg=0):\n",
      "   1. mundo           (similitud: 0.7796)\n",
      "   2. reino           (similitud: 0.7774)\n",
      "   3. sector          (similitud: 0.7650)\n",
      "   4. sistema         (similitud: 0.7637)\n",
      "\n",
      " PALABRAS EN COMÚN: 2/4\n",
      "   ['reino', 'sector']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== FUNCIÓN QUE COMPARACIÓN PARA LA PALABRA: 'psicohistoria' ===\n",
      "\n",
      " SKIP-GRAM (sg=1):\n",
      "   1. predecir        (similitud: 0.7624)\n",
      "   2. validez         (similitud: 0.7321)\n",
      "   3. psicología      (similitud: 0.7252)\n",
      "   4. convención      (similitud: 0.7032)\n",
      "\n",
      " CBOW (sg=0):\n",
      "   1. psicología      (similitud: 0.8444)\n",
      "   2. historia        (similitud: 0.7845)\n",
      "   3. convención      (similitud: 0.7822)\n",
      "   4. ciencia         (similitud: 0.7588)\n",
      "\n",
      " PALABRAS EN COMÚN: 2/4\n",
      "   ['convención', 'psicología']\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== FUNCIÓN QUE COMPARACIÓN PARA LA PALABRA: 'robot' ===\n",
      " La palabra 'robot' no está en el vocabulario de uno o ambos modelos\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# función que realiza comparación de palabras \n",
    "def comparar_modelos(palabra, skipgram_model, cbow_model, topn=10):\n",
    "    \"\"\"\n",
    "    Compara las similitudes de una palabra en ambos modelos\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== FUNCIÓN QUE COMPARACIÓN PARA LA PALABRA: '{palabra}' ===\")\n",
    "    \n",
    "    try:\n",
    "        # Skip-gram\n",
    "        sim_skipgram = skipgram_model.wv.most_similar(palabra, topn=topn)\n",
    "        print(f\"\\n SKIP-GRAM (sg=1):\")\n",
    "        for i, (word, score) in enumerate(sim_skipgram, 1):\n",
    "            print(f\"  {i:2d}. {word:<15} (similitud: {score:.4f})\")\n",
    "        \n",
    "        # CBOW\n",
    "        sim_cbow = cbow_model.wv.most_similar(palabra, topn=topn)\n",
    "        print(f\"\\n CBOW (sg=0):\")\n",
    "        for i, (word, score) in enumerate(sim_cbow, 1):\n",
    "            print(f\"  {i:2d}. {word:<15} (similitud: {score:.4f})\")\n",
    "            \n",
    "        # Palabras en común\n",
    "        words_sg = set([word for word, _ in sim_skipgram])\n",
    "        words_cbow = set([word for word, _ in sim_cbow])\n",
    "        comunes = words_sg.intersection(words_cbow)\n",
    "        \n",
    "        print(f\"\\n PALABRAS EN COMÚN: {len(comunes)}/{topn}\")\n",
    "        print(f\"   {list(comunes)}\")\n",
    "        \n",
    "    except KeyError:\n",
    "        print(f\" La palabra '{palabra}' no está en el vocabulario de uno o ambos modelos\")\n",
    "\n",
    "# Ejemplos de comparación\n",
    "palabras_test = [\"robot\", \"galaxia\", \"planeta\", \"psicohistoria\", \"robot\"]\n",
    "\n",
    "for palabra in palabras_test:\n",
    "    comparar_modelos(palabra, w2v_skipgram, w2v_cbow, topn=4)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g8UVWe6lFmh"
   },
   "source": [
    "### Visualizar agrupación de vectores  (visualización de los embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "pDxEVXAivjr9"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    \n",
    "from sklearn.manifold import TSNE                   \n",
    "import numpy as np                                  \n",
    "\n",
    "def reduce_dimensions(model, num_dimensions = 2 ):\n",
    "     \n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  \n",
    "    # reducción de dimensionalidad TSNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    return vectors, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "NCCXtDpcugmd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\" integrity=\"sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=\" crossorigin=\"anonymous\"></script>                <div id=\"dc3ae091-f65a-4f37-8d41-8dbd01d66462\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"dc3ae091-f65a-4f37-8d41-8dbd01d66462\")) {                    Plotly.newPlot(                        \"dc3ae091-f65a-4f37-8d41-8dbd01d66462\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers+text\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"text\":[\"de\",\"la\",\"que\",\"el\",\"y\",\"en\",\"a\",\"se\",\"no\",\"un\",\"los\",\"con\",\"una\",\"su\",\"lo\",\"por\",\"del\",\"las\",\"es\",\"al\",\"para\",\"más\",\"pero\",\"sus\",\"fundación\",\"era\",\"como\",\"si\",\"me\",\"le\",\"había\",\"ha\",\"sin\",\"qué\",\"todo\",\"cuando\",\"mi\",\"mulo\",\"eso\",\"ya\",\"vez\",\"—no\",\"él\",\"te\",\"usted\",\"ni\",\"ahora\",\"hasta\",\"seldon\",\"nada\",\"sobre\",\"sólo\",\"o\",\"dos\",\"está\",\"yo\",\"imperio\",\"nos\",\"algo\",\"tan\",\"estaba\",\"antes\",\"mis\",\"galaxia\",\"años\",\"ser\",\"he\",\"así\",\"fue\",\"segunda\",\"todos\",\"aquí\",\"voz\",\"—dijo\",\"han\",\"podría\",\"mientras\",\"menos\",\"tu\",\"tiempo\",\"son\",\"bien\",\"ese\",\"mismo\",\"cabeza\",\"desde\",\"entre\",\"¿qué\",\"hombre\",\"este\",\"bayta\",\"uno\",\"puede\",\"nave\",\"muy\",\"hardin\",\"ojos\",\"hay\",\"toran\",\"también\",\"durante\",\"ellos\",\"esa\",\"pritcher\",\"momento\",\"sí\",\"e\",\"esta\",\"tiene\",\"mallow\",\"parte\",\"otro\",\"trantor\",\"esto\",\"planeta\",\"arcadia\",\"hecho\",\"otra\",\"primera\",\"después\",\"aunque\",\"¿no\",\"nuestra\",\"hacia\",\"mano\",\"tal\",\"kalgan\",\"nosotros\",\"naves\",\"hace\",\"entonces\",\"palabras\",\"mundo\",\"sido\",\"ante\",\"sé\",\"contra\",\"darell\",\"ella\",\"primer\",\"nadie\",\"channis\",\"todas\",\"señor\",\"tenía\",\"siempre\",\"anacreonte\",\"hacer\",\"aún\",\"vida\",\"toda\",\"donde\",\"ningún\",\"modo\",\"hombres\",\"general\",\"ninguna\",\"mejor\",\"dijo\",\"terminus\",\"hubiera\",\"emperador\",\"podía\",\"les\",\"gran\",\"tres\",\"haber\",\"espacio\",\"cómo\",\"cierto\",\"guerra\",\"plan\",\"—el\",\"allí\",\"fuera\",\"estado\",\"capitán\",\"doctor\",\"mucho\",\"—sí\",\"embargo\",\"tanto\",\"—¿y\",\"nuevo\",\"caso\",\"casi\",\"porque\",\"historia\",\"pues\",\"fin\",\"poco\",\"mirada\",\"decir\",\"habían\",\"eran\",\"incluso\",\"—¿qué\",\"ver\",\"tras\",\"cosas\"],\"x\":{\"dtype\":\"f4\",\"bdata\":\"GqKtwFBnTMExs7ZB6JgRwRf\\u002fdMEXPIPBaXc3P+HdyMEMJtdBF1AcwZ0iI8APEATCr4efwDMhUsGFCwdCtmaGQbYvfcFUUYlALYa9QXvb+j\\u002feI5hB4S4xQdSIvUFCzJTBFCehv4m5wMB\\u002fRitBHKAnQmkdHELTzxhC2vI2QuzEMULlC4JBlVwnQsWw1UHSz4rB15wtQMFimUHv8zhCgGwtQmEEUkGuBgJCZKWkwT4pFkIzWgBCGJgGQUMtBUJzjQLBgaLrQN0VKkL9YOLB0HoiQZRthr3gEV\\u002fBVpQNQlAp+kEw3TLB8FEUQq4c9kH9p2VB4Ukhwtb4fsE94dbB6NjewASdlMEjhKdBZy42QmOkY0Fx71vBCkhZQIgduD9Sb7RBeDIfwurZyUFtVzVC\\u002fEOPQUy0MMKeOYtBpLdHQGZ9bEGK3WhATMChQfiCx0G4MNtBN+0owt7tskG1PurAoy0gQtoQ07\\u002fLL9BBCMsewhQTOj2AEZtBj7S\\u002fQCAc4UEmxgXCgMuKwfE3n0G2lRTC4L\\u002fbQVEFUcHJVslAFFtfQM23+cFzu4NA3N7EQSHLvsEJwDxARwm9QaCDFsIhn5BAPWL0wCCXnj6yMThCKdFIwXMA5MFqZTBCXHX\\u002fvjJLjj+5IaLApKB7QS+LFEJzYgpABO0vwp20F8K95qY\\u002f3NWGQRYNcEHcDXm\\u002f+5AywbKopEGStmTB4Lq0QHdaHkL1M9nBsEENQnvBYEGYFQ7C8j2lwfPrP8F2r8BBqlcUwqN\\u002fFEADUepBnF9IwdFoD0IRbdvAb9HTQV+lmUAAZRJAiQYpwU+vuMFoA4fBoDv+QNPs2b\\u002fytUHB9zL2v5M69UFuISTCd1qiQF63MkIHZ0xAdEQQQQGlGELZNve+0+pjwY2CKkKy8ihA6IUoQsa3xEHjAJ5B5t\\u002f0QDWW3MFuWwLAPGV+QYSZJUJ8SuLBnXyhQdsGB0IAN+xB9c6AQf\\u002fCgEEp1R1CzbmPwR\\u002fj+EENxkfB\\u002fTPnQQVctsAiRKxBTeqvQdpEf0FY1A7CbzYaQhKfNUJ8wWZAkgicQHLHH0IaAdpBw+OfwZ2ByEA=\"},\"xaxis\":\"x\",\"y\":{\"dtype\":\"f4\",\"bdata\":\"5CMwwYCi8UFy5+9A7uVvQZoPT8Hixw5Bep4QQHcbAsEFv4w\\u002fUvygQGlgycE5BdbAa0wTQrAybMAgcsQ\\u002fuX0QQrpljkEyJhLC1F+SQQkzEEGiumTBlwmCwQCFSb\\u002f7hcrBz9QFQoUsXEHSzCBBIfOWwHIdvMEpEMnB9Qx4Px3KdT\\u002feu4y\\u002flYGZwQVHiUEiEuNAQGhIQeD+ckGAJB3B+GHSwCkZ\\u002f0HM3IvB5J8NwUcbvME5C0rBwbCEQVtR60DapjFAGEsjQlvR+8Dx+OhBVowqQVrxI8F0fSLC3MJfQIxJdMGSDHdBoQqhP99JfkAvxnBByMKRQYFTL8FCrdLBWaz2QThrDsLswDdBt3ncPGD8jMFzMw9CT7v5QXB1qcG9VpjB0zf3wMro38EbyLw\\u002f1ZXCvazWOEEEPb9AhtpMQedj9cBJh+HBtaaSwZKfwUG4X4PBlypLwIywrMEzRrzBJIaowYXFHkEMPslBvx96wTHtC8EHl5g+byC2QZIe5kGEpmPBtme6wcqlGsEhinDBojH+Pvb1F8LB1FTBzqMJQv\\u002foHsBDYo1Ae4hlwfJ65kE6FgxCkzkUwRT9e8FR6MZBM4C9QDzHL0E76x3BVViLQYJCVcHNoEFB53YaQjkgGkKgH7RB8ZVAP7sPncHHcwNC3BwtQWUcZkEmOCFCQdKNQbEgTME7uwPCqklCwRLcW7\\u002fX3ebBpDM+QXJgmkH3wgxAwUvbviUGTsFZroLBq1oKwS9TQUE9QfFAcJ6PwVfWDMKzWojB\\u002f97sPnhxvEADHLVBfFnQQJRbTMHFcp5BpEYBQlyGgkHO5gZCed43QLJ8rMHy5iZB+4QfQjWP+D+eK13A783jwAEXFUB6ZyVBwWfmwBV+xsHkQhtC4PUhwoGvYkAVr1\\u002fAGDWjwVzqBECJX6NBb\\u002fkWQvMEtMEDRJJB\\u002ffzMQP1nEEGdRt\\u002fABG45wZmRCb5YTp7BeQSUv+Mjo8G4\\u002fHXB1jmaQCezP8HDBkDBl7heQD7y9EGR0ojAADC1QSO5iUAbdFxBlQTOvyq1sz\\u002f+weDBnpjdQczVpsFrC4rAPpdYv5xNDcI=\"},\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('dc3ae091-f65a-4f37-8d41-8dbd01d66462');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# !pip install plotly\n",
    "# Graficar los embedddings en 2D\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_skipgram)\n",
    "\n",
    "MAX_WORDS=200\n",
    "fig = px.scatter(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], text=labels[:MAX_WORDS])\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<html>\n",
       "<head><meta charset=\"utf-8\" /></head>\n",
       "<body>\n",
       "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
       "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-3.0.1.min.js\" integrity=\"sha256-oy6Be7Eh6eiQFs5M7oXuPxxm9qbJXEtTpfSI93dW16Q=\" crossorigin=\"anonymous\"></script>                <div id=\"514ab6ee-4681-4d85-a129-ce7ca0aaf382\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                window.PLOTLYENV=window.PLOTLYENV || {};                                if (document.getElementById(\"514ab6ee-4681-4d85-a129-ce7ca0aaf382\")) {                    Plotly.newPlot(                        \"514ab6ee-4681-4d85-a129-ce7ca0aaf382\",                        [{\"hovertemplate\":\"x=%{x}\\u003cbr\\u003ey=%{y}\\u003cbr\\u003ez=%{z}\\u003cbr\\u003etext=%{text}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\",\"size\":2},\"mode\":\"markers+text\",\"name\":\"\",\"scene\":\"scene\",\"showlegend\":false,\"text\":[\"de\",\"la\",\"que\",\"el\",\"y\",\"en\",\"a\",\"se\",\"no\",\"un\",\"los\",\"con\",\"una\",\"su\",\"lo\",\"por\",\"del\",\"las\",\"es\",\"al\",\"para\",\"más\",\"pero\",\"sus\",\"fundación\",\"era\",\"como\",\"si\",\"me\",\"le\",\"había\",\"ha\",\"sin\",\"qué\",\"todo\",\"cuando\",\"mi\",\"mulo\",\"eso\",\"ya\",\"vez\",\"—no\",\"él\",\"te\",\"usted\",\"ni\",\"ahora\",\"hasta\",\"seldon\",\"nada\",\"sobre\",\"sólo\",\"o\",\"dos\",\"está\",\"yo\",\"imperio\",\"nos\",\"algo\",\"tan\",\"estaba\",\"antes\",\"mis\",\"galaxia\",\"años\",\"ser\",\"he\",\"así\",\"fue\",\"segunda\",\"todos\",\"aquí\",\"voz\",\"—dijo\",\"han\",\"podría\",\"mientras\",\"menos\",\"tu\",\"tiempo\",\"son\",\"bien\",\"ese\",\"mismo\",\"cabeza\",\"desde\",\"entre\",\"¿qué\",\"hombre\",\"este\",\"bayta\",\"uno\",\"puede\",\"nave\",\"muy\",\"hardin\",\"ojos\",\"hay\",\"toran\",\"también\",\"durante\",\"ellos\",\"esa\",\"pritcher\",\"momento\",\"sí\",\"e\",\"esta\",\"tiene\",\"mallow\",\"parte\",\"otro\",\"trantor\",\"esto\",\"planeta\",\"arcadia\",\"hecho\",\"otra\",\"primera\",\"después\",\"aunque\",\"¿no\",\"nuestra\",\"hacia\",\"mano\",\"tal\",\"kalgan\",\"nosotros\",\"naves\",\"hace\",\"entonces\",\"palabras\",\"mundo\",\"sido\",\"ante\",\"sé\",\"contra\",\"darell\",\"ella\",\"primer\",\"nadie\",\"channis\",\"todas\",\"señor\",\"tenía\",\"siempre\",\"anacreonte\",\"hacer\",\"aún\",\"vida\",\"toda\",\"donde\",\"ningún\",\"modo\",\"hombres\",\"general\",\"ninguna\",\"mejor\",\"dijo\",\"terminus\",\"hubiera\",\"emperador\",\"podía\",\"les\",\"gran\",\"tres\",\"haber\",\"espacio\",\"cómo\",\"cierto\",\"guerra\",\"plan\",\"—el\",\"allí\",\"fuera\",\"estado\",\"capitán\",\"doctor\",\"mucho\",\"—sí\",\"embargo\",\"tanto\",\"—¿y\",\"nuevo\",\"caso\",\"casi\",\"porque\",\"historia\",\"pues\",\"fin\",\"poco\",\"mirada\",\"decir\",\"habían\",\"eran\",\"incluso\",\"—¿qué\",\"ver\",\"tras\",\"cosas\"],\"x\":{\"dtype\":\"f4\",\"bdata\":\"dI41wLu0hcAKxtBBGfCvwJOkK8EwSUfB9gdOP0HdacEE0c9BrEEBQRfOpj9ObqzBBfauwYxzpMAqoINAlW2qQD9a08DnU97AtYK3QYz3LT5cAO9AcaBjQWsUkUEJvufBqkONQG71esDaUnNBxBnSQbD6QUFmY\\u002fJAgFTyQT5o10HiJxBCEMJmQUEVzUFuus3BW7HaPzWmWj7LadhBViS1Qb0nlEHyo4RBpaRUwRL4LEFpQYRBkGocQQmb6UEBctRAOSrUQDzfnkG1cMvBEFpBQa6NlEEXc2jAQKrZQAZBVkHzB+fA\\u002fgYIQrEs1kGoM4RB6+DqwclEisCVXibByJ6rwOCY0sC3tdJBlM79QYzcnEGVYKHBUX4OQb1qxkCKUGO+dUX4wYN6p8BuwfVBjW65QR12DsKFBZtByaUJwKTwp0CBFhRBAMSQQT3t0UBvXS5BsEnHwQZyk0DKKXvBvkSDQV9ogsA4KJtAGSqRwfFCHkHZENpBW5F3Qbw6ZEEawj\\u002fBNy7DwY9qgkHGtpLBvpDFQW95T79TeElBldItQWsD0MExephA3fsGQalgOcGW\\u002fSFBBpOCQW3OncF0\\u002fA9BzlO7vwV1nUFiJMxBPGnswFFdGsHBBxZCY\\u002fS3QJOK2r0\\u002fa2FBcKvEQfB9kUHJ2sVAD2QUwtZvHMLiMQZCDW4eQvP4h0Gzx5\\u002fAC+GHP+ZI2EAm4cbBcngSQaUbMUL2O67BblFFQaW7RkEUPnjBvjVTwWIz5MDeML5BNPO5wVXE6r\\u002fslxpB8sCFwTsU9kF\\u002f7t48PrTzQc73CUHccDjBytIMwX8pjMGL+0pASHHhQFvlpr4PzyDBKKOVP7\\u002frBELb9vjBRAQ1QfcS1UFBaI6+BHXswJFwDEHfnplAUeYmwHft4kFTqTpATMBDQUqGw0G6ScJBLKOhv8Ekv8CmTR5BHmONQZoM8UFbBkbBH6GKQBIRCkKVD+JAmOAOQnnYIUHmjaxBgRlNwcGkh0HheXHBf\\u002ffgQZOG1r\\u002fOcihA0PznQHkllEEmFxnCvxbpQdor7kFa0SlBsM6FQcrifEEdoOlBZVaAwRAIEsA=\"},\"y\":{\"dtype\":\"f4\",\"bdata\":\"Bm4NwPdDG8JsYEQ\\u002fXLfJwWYOvkAaCKfAxmB3P2TH+UCOyKRAoWoewajzjUGU+RFBxFYAwvdzxkBfPwJB98irQR2XxMHXpQFCbPzcwWd\\u002fhcAone1AemvKQbLTgUChCVZBU0gAwlQOBMLRFpXBIV4LQaTZAEJMPApCWHVRQQ5zRUH\\u002fK5vAGagHQv46usDiGJ7Al6kWQSBoqMHeZ23AmWd0QZy0C8I54Z1BPIFwQQkm60HDC8hBpV8PwoQDY79Zky9AMITPwehZm0FjMRDCh8mkwTlKcMCTXqS\\u002fz5z4QUB7SEFyZpDBhO6GQbz3EcGZn+PBGTqzwcHu6UE\\u002fDvpBhAXowT6o3sC\\u002feZc\\u002fSXxOQXu8hkHKaoNAZxsOwjasaEGF0IhBIok4QQKcqUEQUBlBQXqjQQa5mkDXiQXBSmnuQO8kMUEaP9ZBVY+IQC411cALAnhBmhlyQSHIe0FYkSdBIuYLQob8DME4j+DBC7PmQZnzZ0DGJbhBVZ2pwczSysC1JsBBDc9kQXEe2UFP+M9B8E4CQIH1cMC3uB9BEDEgwusgCMFcWLbAntx7QQQVGcFw7ijC4u3HQVJN5kGyEe7BlDTWwF8Ssz+qrnnAxAK8wVxKgUGFYmtADNgpwhcXJ8J\\u002fHITAjYHTPuaX70GH1BTCJibgQF\\u002fEzMBKX8xA1FCBwA2hj0EPKLdBvBJcwRLp6EATFK9BUBjDwY\\u002fKfcGI2IPBCVMmQewN5kCZccdBrepSQSEYKMHSg6E\\u002fQvTWQVEtCUKL2KtBCfcHwfhfjkDKFkjBuV4cvyZEvT\\u002fFpRY\\u002fIb0BwlJmHMHO2jvBpdBYwIsiKEETuZPB7o8hwsams0D7vYpBIGrxwPfXAkEMmbnBFsozwWvPBUKj6R7CBBxEwMdBq0AB6X5Bh1IPQrkDy7\\u002fhYY3Bl02iwbQpRkEc1dI\\u002ft0HLwd8pDcAdkoa\\u002fnU+aQZqXZb\\u002f5gJZBSSxEwJXIAEK+MOVBM782wffnQEFh5nXAKHJZQL7p4MH7fwZCCO+BwNiKisD\\u002fX4zAwEUwQbLaJ0Eta8xBjGcGwpVZB0KHE5FBlQ8HwJ+WCUI=\"},\"z\":{\"dtype\":\"f4\",\"bdata\":\"zzS1QZK2PkE3UmtAK7mKwa9wlkGfkZ9BIp0Nwt5wTsFU7WvBkFn2wWMpEELpeW3Btwk1QcUREcI7HNLBq3XIPzT+OcEfar1B0GFhwY66FcJ3AQnCHmExQQ8WYcFoSKlBcgyyQcaudMEJPVjBlN6vwVphvcHk7JnB8oGmQTUSnUGg1apBd0QbwVl8lsFcT7VBZAwXwhDjYcHGac\\u002fBthHhwRD2LsFuGb3BDxEBwU4ZpsG336rB3iwkwfS0NMG0EOnAi4LbQUci48Eg8Kw\\u002f1utMwdGD4EE5yyRCclgGwEDEncGp7KBBLogBQesF3UFJdUzBkN2CwJz2AsEmdfHAEV6nQVq46UFsQjZBHxqMQWxAhEFEmhLCFp1fQV\\u002f5BUJxkybBpISnwcOtAcIt9JVBmyjeQJCHkUERxc9BOk8SwqHBIr5ai85BOJj\\u002fwYBwCMJQ6s\\u002fBOngjwRUqT8EpkulBiHyMwUlglcHsaInBTt4twfQrDkJYRM5AgbrnQLUZFcL8thjBa+KZQe9120BKbpLBMQ+PwZAq90Fiiw5CmvX+QIZUs8GBqE3B3sBvwdvVX0ENnydBcOR8QCQbf8EgCzS\\u002fHoHawSWDq0FkSsvB8NsPP0zPJsGR7A1BxtqMP\\u002f98KEBqqQVC0o2yQc\\u002fpJsGO1CZBD7FtQW+h9kD\\u002fEZXBpH6Awcx450B7spJB9mgIQimYycDdqyJB5MmDwQz43r4KV13BcbMlwcJWIsI4JPLAZPoDwYqAXUEBEdRA0PCPwSCZrkGtP4jBVWDzwVxyPD\\u002foqqZBghJdQViRtEGdODpA+jtTQR484kEZiQHCAVpowdVcBULXIUjBNjuZwLN7QsGEBi3BkErDQfzFqUH0ParBPskLwlXemsGynmS\\u002fJGolQg5ZYUH822u\\u002fNQZawQIFhsGBwpRBbh3GQRTh9sEl3xbCvULBwdFG5D9bZMTBGD4TwVhDf8Et2sHB9K6ZQcCJn0DaVJDAY1krwX2258GWZMVBvcG2wS6jg0HKAqPAPoASwQrDB8L04jo+jWawQCF1qUGs\\u002fM5BwNyvv4w6gsFHb24\\u002fhMgeQYJvYEE=\"},\"type\":\"scatter3d\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermap\":[{\"type\":\"scattermap\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"xaxis\":{\"title\":{\"text\":\"x\"}},\"yaxis\":{\"title\":{\"text\":\"y\"}},\"zaxis\":{\"title\":{\"text\":\"z\"}}},\"legend\":{\"tracegroupgap\":0},\"margin\":{\"t\":60}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('514ab6ee-4681-4d85-a129-ce7ca0aaf382');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };            </script>        </div>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Graficar los embedddings en 3D\n",
    "\n",
    "vecs, labels = reduce_dimensions(w2v_skipgram,3)\n",
    "\n",
    "fig = px.scatter_3d(x=vecs[:MAX_WORDS,0], y=vecs[:MAX_WORDS,1], z=vecs[:MAX_WORDS,2],text=labels[:MAX_WORDS])\n",
    "fig.update_traces(marker_size = 2)\n",
    "fig.show(renderer=\"colab\") # esto para plotly en colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Ve a: http://projector.tensorflow.org/\n"
     ]
    }
   ],
   "source": [
    "#import webbrowser\n",
    "## También se pueden guardar los vectores y labels como tsv para graficar en\n",
    "## http://projector.tensorflow.org/\n",
    "#\n",
    "#modelo = w2v_skipgram\n",
    "#modelo = w2v_cbow\n",
    "#\n",
    "#vectors = np.asarray(modelo.wv.vectors)\n",
    "#labels = list(modelo.wv.index_to_key)\n",
    "#\n",
    "#np.savetxt(\"vectors.tsv\", vectors, delimiter=\"\\t\")\n",
    "#\n",
    "#with open(\"labels-metadata.tsv\", \"w\") as fp:\n",
    "#    for item in labels:\n",
    "#        fp.write(\"%s\\n\" % item)\n",
    "#\n",
    "##--------------\n",
    "### Abrir automáticamente TensorBoard Projector\n",
    "##webbrowser.open(\"http://projector.tensorflow.org/\")\n",
    "#\n",
    "print(\"1. Ve a: http://projector.tensorflow.org/\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
